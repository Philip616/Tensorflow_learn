{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Philip\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gameOb():\n",
    "    def __init__(self,coordinates,size,intensity,channel,reward,name):       \n",
    "        self.x = coordinates[0]\n",
    "        self.y = coordinates[1]\n",
    "        self.size = size\n",
    "        self.intensity = intensity\n",
    "        self.channel = channel\n",
    "        self.reward = reward\n",
    "        self.name = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gameEnv():\n",
    "    def __init__(self, size):\n",
    "        self.sizeX = size\n",
    "        self.sizeY = size\n",
    "        self.actions = 4\n",
    "        self.objects = []\n",
    "        a = self.reset()\n",
    "        plt.imshow(a, interpolation=\"nearest\")\n",
    "        \n",
    "    def reset(self):\n",
    "        self.objects = []\n",
    "        hero = gameOb(self.newPosition(),1,1,2,None,'hero')\n",
    "        self.objects.append(hero)\n",
    "        \n",
    "        goal = gameOb(self.newPosition(),1,1,1,1,'goal')\n",
    "        self.objects.append(goal)\n",
    "        \n",
    "        hole = gameOb(self.newPosition(),1,1,0,-1,'fire')\n",
    "        self.objects.append(hole)\n",
    "        \n",
    "        goal2 = gameOb(self.newPosition(),1,1,1,1,'goal')\n",
    "        self.objects.append(goal2)\n",
    "        \n",
    "        hole2 = gameOb(self.newPosition(),1,1,0,-1,'fire')\n",
    "        self.objects.append(hole2)\n",
    "        \n",
    "        goal3 = gameOb(self.newPosition(),1,1,1,1,'goal')\n",
    "        self.objects.append(goal3)\n",
    "        \n",
    "        goal4 = gameOb(self.newPosition(),1,1,1,1,'goal')\n",
    "        self.objects.append(goal4)\n",
    "        \n",
    "        state = self.renderEnv()\n",
    "        self.state = state\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def moveChar(self,direction):\n",
    "        hero = self.objects[0]\n",
    "        heroX = hero.x\n",
    "        heroY = hero.y\n",
    "        \n",
    "        if direction == 0 and hero.y >= 1:\n",
    "            hero.y -= 1\n",
    "        if direction == 1 and hero.y <= self.sizeY - 2:\n",
    "            hero.y += 1\n",
    "        if direction == 2 and hero.x >= 1:\n",
    "            hero.x -= 1\n",
    "        if direction == 3 and hero.x <= self.sizeX - 2:\n",
    "            hero.x += 1\n",
    "            \n",
    "    def newPosition(self):\n",
    "        iterables = [ range(self.sizeX), range(self.sizeY)]\n",
    "        points = []\n",
    "        \n",
    "        for t in itertools.product(*iterables):\n",
    "            points.append(t)\n",
    "        currentPositions = []\n",
    "        \n",
    "        for objectA in self.objects:\n",
    "            if(objectA.x, objectA.y) not in currentPositions:\n",
    "                currentPositions.append((objectA.x, objectA.y))\n",
    "                \n",
    "        for pos in currentPositions:\n",
    "            points.remove(pos)\n",
    "        \n",
    "        location = np.random.choice(range(len(points)), replace=False)\n",
    "        return points[location]\n",
    "    \n",
    "    def checkGoal(self):\n",
    "        others = []\n",
    "        for obj in self.objects:\n",
    "            if obj.name == 'hero':\n",
    "                hero = obj\n",
    "            else:\n",
    "                others.append(obj)\n",
    "        \n",
    "        for other in others:\n",
    "            if hero.x == other.x and hero.y == other.y:\n",
    "                self.objects.remove(other)\n",
    "                if other.reward == 1:\n",
    "                    self.objects.append(gameOb(self.newPosition(),1,1,1,1,'goal'))\n",
    "                else:\n",
    "                    self.objects.append(gameOb(self.newPosition(),1,1,0,-1,'fire'))\n",
    "                    \n",
    "                return other.reward,False\n",
    "        return 0.0,False\n",
    "    \n",
    "    def renderEnv(self):\n",
    "        a = np.ones([self.sizeY+2,self.sizeX+2,3])\n",
    "        a[1:-1,1:-1,:] = 0\n",
    "        hero = None\n",
    "        \n",
    "        for item in self.objects:\n",
    "            a[item.y+1:item.y+item.size+1, item.x+1:item.x+item.size+1, item.channel] = item.intensity\n",
    "            \n",
    "        b = scipy.misc.imresize(a[:,:,0], [84,84,1], interp='nearest')\n",
    "        c = scipy.misc.imresize(a[:,:,1], [84,84,1], interp='nearest')\n",
    "        d = scipy.misc.imresize(a[:,:,2], [84,84,1], interp='nearest')\n",
    "        a = np.stack([b,c,d],axis=2)\n",
    "        \n",
    "        return a\n",
    "    \n",
    "    def step(self,action):\n",
    "        self.moveChar(action)\n",
    "        reward,done = self.checkGoal()\n",
    "        state = self.renderEnv()\n",
    "        return state,reward,done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Philip\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:97: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "C:\\Users\\Philip\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:98: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "C:\\Users\\Philip\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:99: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAADNFJREFUeJzt3V+MHeV5x/HvrzaEQBoZ808uhi5IiIAqYahFoVRVCqGlJIJeJBUoqqIKiZu0hSZSAu0FitQLIlUJuagioZAUVZQ/IdAgKyK1HKKqNw7mTxPAEAxxYQvBJoWSJlJbJ08vZqxunTU76z1nzw7v9yOtzpn3nKN5x6Pfzpzx7POkqpDUll+a9QQkrT6DLzXI4EsNMvhSgwy+1CCDLzXI4EsNWlHwk1yR5Lkke5LcNKlJSZquHOkNPEnWAd8HLgfmgUeBa6vqmclNT9I0rF/BZy8E9lTViwBJ7gGuBg4b/BNPPLHm5uZWsEpJb2fv3r28/vrrWep9Kwn+qcDLC5bngd94uw/Mzc2xa9euFaxS0tvZunXroPet5Dv+Yr9VfuF7Q5Lrk+xKsmv//v0rWJ2kSVlJ8OeB0xYsbwZeOfRNVXV7VW2tqq0nnXTSClYnaVJWEvxHgbOSnJHkaOAa4KHJTEvSNB3xd/yqOpDkT4BvAuuAL1fV0xObmaSpWcnFParqG8A3JjQXSavEO/ekBhl8qUEGX2qQwZcaZPClBhl8qUEGX2qQwZcaZPClBhl8qUEGX2qQwZcaZPClBhl8qUEGX2qQwZcaZPClBi0Z/CRfTrIvyVMLxjYm2Z7k+f7x+OlOU9IkDTni/y1wxSFjNwE7quosYEe/LGkklgx+Vf0T8O+HDF8N3Nk/vxP4gwnPS9IUHel3/FOq6lWA/vHkyU1J0rRN/eKenXSktedIg/9akk0A/eO+w73RTjrS2nOkwX8I+Fj//GPA1yczHUmrYcmGGknuBt4PnJhkHrgFuBW4L8l1wEvAR6Y5yUlIluwc/M70C21MV1mr/+w163/4t7dk8Kvq2sO8dNmE5yJplXjnntQggy81yOBLDTL4UoMMvtQggy81yOBLDTL4UoMMvtQggy81yOBLDTL4UoMMvtQggy81yOBLDTL4UoMMvtSgIZ10TkvySJLdSZ5OckM/bjcdaaSGHPEPAJ+sqnOAi4CPJzkXu+lIozWkk86rVfV4//zHwG7gVOymI43Wsr7jJ5kDzgd2MrCbjg01pLVncPCTvAf4GnBjVb019HM21JDWnkHBT3IUXejvqqoH+uHB3XQkrS1DruoHuAPYXVWfW/CS3XSkkVqyoQZwCfBHwPeSPNmP/QUj7KYjqTOkk84/c/hGSHbTkUbIO/ekBhl8qUEGX2rQkIt7GrGacZvqma5+bXeqnimP+FKDDL7UIIMvNcjgSw0y+FKDDL7UIIMvNcjgSw0y+FKDDL7UIIMvNcjgSw0y+FKDhtTcOybJd5L8S99J5zP9+BlJdvaddO5NcvT0pytpEoYc8f8LuLSqzgO2AFckuQj4LPD5vpPOG8B105umpEka0kmnquo/+8Wj+p8CLgXu78ftpCONyNC6+uv6Crv7gO3AC8CbVXWgf8s8XVutxT5rJx1pjRkU/Kr6WVVtATYDFwLnLPa2w3zWTjrSGrOsq/pV9SbwbbquuRuSHCzdtRl4ZbJTkzQtQ67qn5RkQ//83cAH6DrmPgJ8uH+bnXSkERlSbHMTcGeSdXS/KO6rqm1JngHuSfJXwBN0bbYkjcCQTjrfpWuNfej4i3Tf9yWNjHfuSQ0y+FKDDL7UIIMvNcjgSw0y+FKDDL7UIIMvNcg22e9wM+6SPVuz3Pg13qLbI77UIIMvNcjgSw0y+FKDDL7UIIMvNcjgSw0y+FKDBge/L7H9RJJt/bKddKSRWs4R/wa6IpsH2UlHGqmhDTU2Ax8EvtQvBzvpSKM19Ih/G/Ap4Of98gnYSUcarSF19T8E7KuqxxYOL/JWO+lIIzHkr/MuAa5KciVwDPBeujOADUnW90d9O+lIIzKkW+7NVbW5quaAa4BvVdVHsZOONFor+X/8TwOfSLKH7ju/nXSkkVhWIY6q+jZd00w76Ugj5p17UoMMvtQggy81yOBLDTL4UoMMvtQggy81yOBLDTL4UoMMvtQggy81yOBLDTL4UoMMvtSgZf1Z7qjNsl95ZrryGa5ba5VHfKlBg474SfYCPwZ+Bhyoqq1JNgL3AnPAXuAPq+qN6UxT0iQt54j/O1W1paq29ss3ATv6hho7+mVJI7CSU/2r6RppgA01pFEZGvwC/jHJY0mu78dOqapXAfrHk6cxQUmTN/Sq/iVV9UqSk4HtSZ4duoL+F8X1AKeffvoRTFHSpA064lfVK/3jPuBBuuq6ryXZBNA/7jvMZ+2kI60xQ1poHZfklw8+B34XeAp4iK6RBthQQxqVIaf6pwAPdg1yWQ/8fVU9nORR4L4k1wEvAR+Z3jQlTdKSwe8bZ5y3yPiPgMumMSlJ0+Wde1KDDL7UIIMvNcjgSw0y+FKDDL7UIIMvNcjgSw0y+FKDDL7UIIMvNcjgSw0y+FKDDL7UIIMvNcjgSw0y+FKDBgU/yYYk9yd5NsnuJBcn2Zhke5Ln+8fjpz1ZSZMx9Ij/BeDhqnofXRmu3dhJRxqtIVV23wv8NnAHQFX9d1W9iZ10pNEaUmX3TGA/8JUk5wGPATdwSCedvtmGFtVuq2obhK9NQ0711wMXAF+sqvOBn7CM0/ok1yfZlWTX/v37j3CakiZpSPDngfmq2tkv30/3i8BOOtJILRn8qvoh8HKSs/uhy4BnsJOONFpDm2b+KXBXkqOBF4E/pvulYScdaYQGBb+qngS2LvKSnXSkEfLOPalBBl9qkMGXGmTwpQYZfKlBBl9qkMGXGmTwpQYZfKlBBl9qkMGXGmTwpQYZfKlBBl9qkMGXGmTwpQYZfKlBQ+rqn53kyQU/byW50U460ngNKbb5XFVtqaotwK8DPwUexE460mgt91T/MuCFqvpX7KQjjdZyg38NcHf//P910gHspCONxODg96W1rwK+upwV2ElHWnuWc8T/feDxqnqtX7aTjjRSywn+tfzfaT7YSUcarUHBT3IscDnwwILhW4HLkzzfv3br5KcnaRqGdtL5KXDCIWM/YkSddGqWDZtn2Su6Yf6zH5537kkNMvhSgwy+1CCDLzXI4EsNMvhSgwy+1CCDLzXI4EsNMvhSgwy+1CCDLzXI4EsNMvhSgwy+1CCDLzXI4EsNGlp668+TPJ3kqSR3JzkmyRlJdvaddO7tq/BKGoEhLbROBf4M2FpVvwaso6uv/1ng830nnTeA66Y5UUmTM/RUfz3w7iTrgWOBV4FLgfv71+2kI43IkN55/wb8NfASXeD/A3gMeLOqDvRvmwdOndYkJU3WkFP94+n65J0B/ApwHF1zjUMtWtTUTjrS2jPkVP8DwA+qan9V/Q9dbf3fBDb0p/4Am4FXFvuwnXSktWdI8F8CLkpybJLQ1dJ/BngE+HD/HjvpSCMy5Dv+TrqLeI8D3+s/czvwaeATSfbQNdu4Y4rzlDRBQzvp3ALccsjwi8CFE5+RpKnzzj2pQQZfapDBlxpk8KUGpWr1mgkn2Q/8BHh91VY6fSfi9qxV76RtgWHb86tVteQNM6safIAku6pq66qudIrcnrXrnbQtMNnt8VRfapDBlxo0i+DfPoN1TpPbs3a9k7YFJrg9q/4dX9LseaovNWhVg5/kiiTPJdmT5KbVXPdKJTktySNJdvf1B2/oxzcm2d7XHtze1y8YjSTrkjyRZFu/PNpaikk2JLk/ybP9frp4zPtnmrUuVy34SdYBf0NXxONc4Nok567W+ifgAPDJqjoHuAj4eD//m4Adfe3BHf3ymNwA7F6wPOZail8AHq6q9wHn0W3XKPfP1GtdVtWq/AAXA99csHwzcPNqrX8K2/N14HLgOWBTP7YJeG7Wc1vGNmymC8OlwDYgdDeIrF9sn63lH+C9wA/or1stGB/l/qErZfcysJHur2i3Ab83qf2zmqf6BzfkoNHW6UsyB5wP7AROqapXAfrHk2c3s2W7DfgU8PN++QTGW0vxTGA/8JX+q8uXkhzHSPdPTbnW5WoGP4uMje6/FJK8B/gacGNVvTXr+RypJB8C9lXVYwuHF3nrWPbReuAC4ItVdT7dreGjOK1fzEprXS5lNYM/D5y2YPmwdfrWqiRH0YX+rqp6oB9+Lcmm/vVNwL5ZzW+ZLgGuSrIXuIfudP82BtZSXIPmgfnqKkZBVzXqAsa7f1ZU63Ipqxn8R4Gz+quSR9NdqHhoFde/In29wTuA3VX1uQUvPURXcxBGVHuwqm6uqs1VNUe3L75VVR9lpLUUq+qHwMtJzu6HDtaGHOX+Ydq1Llf5gsWVwPeBF4C/nPUFlGXO/bfoTqu+CzzZ/1xJ9714B/B8/7hx1nM9gm17P7Ctf34m8B1gD/BV4F2znt8ytmMLsKvfR/8AHD/m/QN8BngWeAr4O+Bdk9o/3rknNcg796QGGXypQQZfapDBlxpk8KUGGXypQQZfapDBlxr0v0P/5HgPqP/MAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20b5539db00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gameEnv(size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self,h_size):\n",
    "        self.scalerInput = tf.placeholder(tf.float32, shape=[None,21168])\n",
    "        self.imageIn = tf.reshape(self.scalerInput, shape=[-1,84,84,3])\n",
    "        \n",
    "        self.conv1 = tf.contrib.layers.convolution2d(inputs=self.imageIn, num_outputs=32,\n",
    "                                                     kernel_size=[8,8], stride=[4,4], padding='VALID', biases_initializer=None)\n",
    "        self.conv2 = tf.contrib.layers.convolution2d(inputs=self.conv1, num_outputs=64,\n",
    "                                                     kernel_size=[4,4], stride=[2,2], padding='VALID', biases_initializer=None)\n",
    "        self.conv3 = tf.contrib.layers.convolution2d(inputs=self.conv2, num_outputs=64,\n",
    "                                                     kernel_size=[3,3], stride=[1,1], padding='VALID', biases_initializer=None)\n",
    "        self.conv4 = tf.contrib.layers.convolution2d(inputs=self.conv3, num_outputs=512,\n",
    "                                                     kernel_size=[7,7], stride=[1,1], padding='VALID', biases_initializer=None)\n",
    "        \n",
    "        #將conv4拆成DQN的Advantage Function和Value Function\n",
    "        self.streamAC, self.streamVC = tf.split(self.conv4, 2, 3) #第二個參數是要拆幾段，第三個是要拆第幾個維度\n",
    "        self.streamA = tf.contrib.layers.flatten(self.streamAC) \n",
    "        self.streamV = tf.contrib.layers.flatten(self.streamVC)\n",
    "        \n",
    "        #創建Fully connect\n",
    "        self.AW = tf.Variable(tf.random_normal([h_size//2, env.actions]))   #Action數量\n",
    "        self.VW = tf.Variable(tf.random_normal([h_size//2, 1]))\n",
    "        self.Advantage = tf.matmul(self.streamA, self.AW)\n",
    "        self.Value = tf.matmul(self.streamV, self.VW)\n",
    "        \n",
    "        #Q值\n",
    "        self.Qout = self.Value + tf.subtract(self.Advantage, tf.reduce_mean(self.Advantage, reduction_indices=1,keep_dims=True))\n",
    "        self.predict = tf.argmax(self.Qout, 1)\n",
    "        \n",
    "        self.targetQ = tf.placeholder(tf.float32,shape=[None])\n",
    "        self.actions = tf.placeholder(tf.int32,shape=[None])\n",
    "        self.action_onehot = tf.one_hot(self.actions, env.actions, dtype=tf.float32)\n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.action_onehot), reduction_indices=1)\n",
    "        \n",
    "        #loss計算targetQ跟Q的均方誤差\n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        self.loss = tf.reduce_mean(self.td_error)\n",
    "        self.trainer = tf.train.AdamOptimizer(0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "        \n",
    "    def add(self, experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer)) - self.buffer_size] = []\n",
    "            \n",
    "        self.buffer.extend(experience)\n",
    "        \n",
    "    def sample(self, size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer, size)), [size,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processState(states):\n",
    "    return np.reshape(states,[21168])\n",
    "    \n",
    "#更新Targer DQN\n",
    "def updateTargetGraph(tfVars, tau):\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "        \n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(tfVars[idx + total_vars//2].assign((var.value() * tau) + \\\n",
    "                                                                ((1 - tau) * tfVars[idx+total_vars//2].value())))\n",
    "    return op_holder\n",
    "    \n",
    "def updateTarget(op_holder, sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Philip\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:97: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "C:\\Users\\Philip\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:98: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "C:\\Users\\Philip\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:99: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 25  average reward of last 25 episode 2.44\n",
      "episode 50  average reward of last 25 episode 2.64\n",
      "episode 75  average reward of last 25 episode 1.6\n",
      "episode 100  average reward of last 25 episode 1.44\n",
      "episode 125  average reward of last 25 episode 2.24\n",
      "episode 150  average reward of last 25 episode 1.92\n",
      "episode 175  average reward of last 25 episode 2.16\n",
      "episode 200  average reward of last 25 episode 1.04\n",
      "episode 225  average reward of last 25 episode 2.2\n",
      "episode 250  average reward of last 25 episode 1.56\n",
      "episode 275  average reward of last 25 episode 1.84\n",
      "episode 300  average reward of last 25 episode 0.96\n",
      "episode 325  average reward of last 25 episode 1.48\n",
      "episode 350  average reward of last 25 episode 1.92\n",
      "episode 375  average reward of last 25 episode 0.68\n",
      "episode 400  average reward of last 25 episode 0.68\n",
      "episode 425  average reward of last 25 episode 0.96\n",
      "episode 450  average reward of last 25 episode 1.32\n",
      "episode 475  average reward of last 25 episode 1.36\n",
      "episode 500  average reward of last 25 episode 0.68\n",
      "episode 525  average reward of last 25 episode 0.84\n",
      "episode 550  average reward of last 25 episode 0.96\n",
      "episode 575  average reward of last 25 episode 1.04\n",
      "episode 600  average reward of last 25 episode 0.96\n",
      "episode 625  average reward of last 25 episode 1.52\n",
      "episode 650  average reward of last 25 episode 0.76\n",
      "episode 675  average reward of last 25 episode 1.36\n",
      "episode 700  average reward of last 25 episode 1.28\n",
      "episode 725  average reward of last 25 episode 0.92\n",
      "episode 750  average reward of last 25 episode 1.52\n",
      "episode 775  average reward of last 25 episode 1.12\n",
      "episode 800  average reward of last 25 episode 2.2\n",
      "episode 825  average reward of last 25 episode 1.44\n",
      "episode 850  average reward of last 25 episode 1.6\n",
      "episode 875  average reward of last 25 episode 1.48\n",
      "episode 900  average reward of last 25 episode 1.6\n",
      "episode 925  average reward of last 25 episode 1.4\n",
      "episode 950  average reward of last 25 episode 1.64\n",
      "episode 975  average reward of last 25 episode 1.2\n",
      "episode 1000  average reward of last 25 episode 0.84\n",
      "Saved Model\n",
      "episode 1025  average reward of last 25 episode 1.6\n",
      "episode 1050  average reward of last 25 episode 1.88\n",
      "episode 1075  average reward of last 25 episode 1.32\n",
      "episode 1100  average reward of last 25 episode 2.44\n",
      "episode 1125  average reward of last 25 episode 2.68\n",
      "episode 1150  average reward of last 25 episode 1.8\n",
      "episode 1175  average reward of last 25 episode 1.68\n",
      "episode 1200  average reward of last 25 episode 2.08\n",
      "episode 1225  average reward of last 25 episode 2.6\n",
      "episode 1250  average reward of last 25 episode 2.32\n",
      "episode 1275  average reward of last 25 episode 2.48\n",
      "episode 1300  average reward of last 25 episode 1.56\n",
      "episode 1325  average reward of last 25 episode 2.8\n",
      "episode 1350  average reward of last 25 episode 2.44\n",
      "episode 1375  average reward of last 25 episode 2.44\n",
      "episode 1400  average reward of last 25 episode 2.2\n",
      "episode 1425  average reward of last 25 episode 3.32\n",
      "episode 1450  average reward of last 25 episode 4.28\n",
      "episode 1475  average reward of last 25 episode 3.96\n",
      "episode 1500  average reward of last 25 episode 4.36\n",
      "episode 1525  average reward of last 25 episode 3.32\n",
      "episode 1550  average reward of last 25 episode 3.68\n",
      "episode 1575  average reward of last 25 episode 3.88\n",
      "episode 1600  average reward of last 25 episode 3.68\n",
      "episode 1625  average reward of last 25 episode 4.8\n",
      "episode 1650  average reward of last 25 episode 3.12\n",
      "episode 1675  average reward of last 25 episode 4.92\n",
      "episode 1700  average reward of last 25 episode 5.16\n",
      "episode 1725  average reward of last 25 episode 6.76\n",
      "episode 1750  average reward of last 25 episode 5.96\n",
      "episode 1775  average reward of last 25 episode 9.28\n",
      "episode 1800  average reward of last 25 episode 7.96\n",
      "episode 1825  average reward of last 25 episode 6.32\n",
      "episode 1850  average reward of last 25 episode 8.72\n",
      "episode 1875  average reward of last 25 episode 9.28\n",
      "episode 1900  average reward of last 25 episode 12.0\n",
      "episode 1925  average reward of last 25 episode 11.28\n",
      "episode 1950  average reward of last 25 episode 10.24\n",
      "episode 1975  average reward of last 25 episode 10.92\n",
      "episode 2000  average reward of last 25 episode 10.72\n",
      "Saved Model\n",
      "episode 2025  average reward of last 25 episode 10.84\n",
      "episode 2050  average reward of last 25 episode 14.76\n",
      "episode 2075  average reward of last 25 episode 13.68\n",
      "episode 2100  average reward of last 25 episode 12.4\n",
      "episode 2125  average reward of last 25 episode 12.24\n",
      "episode 2150  average reward of last 25 episode 15.08\n",
      "episode 2175  average reward of last 25 episode 14.44\n",
      "episode 2200  average reward of last 25 episode 14.92\n",
      "episode 2225  average reward of last 25 episode 15.4\n",
      "episode 2250  average reward of last 25 episode 12.24\n",
      "episode 2275  average reward of last 25 episode 16.0\n",
      "episode 2300  average reward of last 25 episode 14.56\n",
      "episode 2325  average reward of last 25 episode 13.36\n",
      "episode 2350  average reward of last 25 episode 15.68\n",
      "episode 2375  average reward of last 25 episode 16.92\n",
      "episode 2400  average reward of last 25 episode 19.32\n",
      "episode 2425  average reward of last 25 episode 17.24\n",
      "episode 2450  average reward of last 25 episode 18.04\n",
      "episode 2475  average reward of last 25 episode 18.16\n",
      "episode 2500  average reward of last 25 episode 18.4\n",
      "episode 2525  average reward of last 25 episode 18.92\n",
      "episode 2550  average reward of last 25 episode 14.8\n",
      "episode 2575  average reward of last 25 episode 17.8\n",
      "episode 2600  average reward of last 25 episode 18.56\n",
      "episode 2625  average reward of last 25 episode 17.76\n",
      "episode 2650  average reward of last 25 episode 17.16\n",
      "episode 2675  average reward of last 25 episode 19.4\n",
      "episode 2700  average reward of last 25 episode 18.84\n",
      "episode 2725  average reward of last 25 episode 17.44\n",
      "episode 2750  average reward of last 25 episode 19.16\n",
      "episode 2775  average reward of last 25 episode 18.52\n",
      "episode 2800  average reward of last 25 episode 20.12\n",
      "episode 2825  average reward of last 25 episode 19.04\n",
      "episode 2850  average reward of last 25 episode 18.72\n",
      "episode 2875  average reward of last 25 episode 20.0\n",
      "episode 2900  average reward of last 25 episode 19.64\n",
      "episode 2925  average reward of last 25 episode 20.32\n",
      "episode 2950  average reward of last 25 episode 19.04\n",
      "episode 2975  average reward of last 25 episode 20.08\n",
      "episode 3000  average reward of last 25 episode 18.28\n",
      "Saved Model\n",
      "episode 3025  average reward of last 25 episode 20.68\n",
      "episode 3050  average reward of last 25 episode 20.84\n",
      "episode 3075  average reward of last 25 episode 20.84\n",
      "episode 3100  average reward of last 25 episode 20.0\n",
      "episode 3125  average reward of last 25 episode 20.28\n",
      "episode 3150  average reward of last 25 episode 20.64\n",
      "episode 3175  average reward of last 25 episode 21.12\n",
      "episode 3200  average reward of last 25 episode 20.0\n",
      "episode 3225  average reward of last 25 episode 20.24\n",
      "episode 3250  average reward of last 25 episode 20.96\n",
      "episode 3275  average reward of last 25 episode 19.56\n",
      "episode 3300  average reward of last 25 episode 20.2\n",
      "episode 3325  average reward of last 25 episode 20.72\n",
      "episode 3350  average reward of last 25 episode 20.08\n",
      "episode 3375  average reward of last 25 episode 22.72\n",
      "episode 3400  average reward of last 25 episode 19.52\n",
      "episode 3425  average reward of last 25 episode 20.64\n",
      "episode 3450  average reward of last 25 episode 20.24\n",
      "episode 3475  average reward of last 25 episode 20.32\n",
      "episode 3500  average reward of last 25 episode 20.72\n",
      "episode 3525  average reward of last 25 episode 20.36\n",
      "episode 3550  average reward of last 25 episode 20.28\n",
      "episode 3575  average reward of last 25 episode 21.08\n",
      "episode 3600  average reward of last 25 episode 21.08\n",
      "episode 3625  average reward of last 25 episode 21.2\n",
      "episode 3650  average reward of last 25 episode 19.52\n",
      "episode 3675  average reward of last 25 episode 21.84\n",
      "episode 3700  average reward of last 25 episode 22.04\n",
      "episode 3725  average reward of last 25 episode 20.8\n",
      "episode 3750  average reward of last 25 episode 18.8\n",
      "episode 3775  average reward of last 25 episode 21.64\n",
      "episode 3800  average reward of last 25 episode 22.96\n",
      "episode 3825  average reward of last 25 episode 21.68\n",
      "episode 3850  average reward of last 25 episode 21.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3875  average reward of last 25 episode 22.32\n",
      "episode 3900  average reward of last 25 episode 21.4\n",
      "episode 3925  average reward of last 25 episode 21.76\n",
      "episode 3950  average reward of last 25 episode 21.0\n",
      "episode 3975  average reward of last 25 episode 22.76\n",
      "episode 4000  average reward of last 25 episode 23.2\n",
      "Saved Model\n",
      "episode 4025  average reward of last 25 episode 21.64\n",
      "episode 4050  average reward of last 25 episode 21.12\n",
      "episode 4075  average reward of last 25 episode 21.36\n",
      "episode 4100  average reward of last 25 episode 20.92\n",
      "episode 4125  average reward of last 25 episode 21.88\n",
      "episode 4150  average reward of last 25 episode 22.6\n",
      "episode 4175  average reward of last 25 episode 21.92\n",
      "episode 4200  average reward of last 25 episode 20.48\n",
      "episode 4225  average reward of last 25 episode 20.88\n",
      "episode 4250  average reward of last 25 episode 22.04\n",
      "episode 4275  average reward of last 25 episode 21.64\n",
      "episode 4300  average reward of last 25 episode 21.32\n",
      "episode 4325  average reward of last 25 episode 21.88\n",
      "episode 4350  average reward of last 25 episode 21.44\n",
      "episode 4375  average reward of last 25 episode 20.84\n",
      "episode 4400  average reward of last 25 episode 21.16\n",
      "episode 4425  average reward of last 25 episode 20.92\n",
      "episode 4450  average reward of last 25 episode 22.24\n",
      "episode 4475  average reward of last 25 episode 22.08\n",
      "episode 4500  average reward of last 25 episode 22.8\n",
      "episode 4525  average reward of last 25 episode 22.16\n",
      "episode 4550  average reward of last 25 episode 21.52\n",
      "episode 4575  average reward of last 25 episode 21.36\n",
      "episode 4600  average reward of last 25 episode 21.28\n",
      "episode 4625  average reward of last 25 episode 21.32\n",
      "episode 4650  average reward of last 25 episode 21.96\n",
      "episode 4675  average reward of last 25 episode 22.32\n",
      "episode 4700  average reward of last 25 episode 21.08\n",
      "episode 4725  average reward of last 25 episode 22.56\n",
      "episode 4750  average reward of last 25 episode 21.2\n",
      "episode 4775  average reward of last 25 episode 21.52\n",
      "episode 4800  average reward of last 25 episode 22.48\n",
      "episode 4825  average reward of last 25 episode 21.6\n",
      "episode 4850  average reward of last 25 episode 21.08\n",
      "episode 4875  average reward of last 25 episode 21.76\n",
      "episode 4900  average reward of last 25 episode 22.36\n",
      "episode 4925  average reward of last 25 episode 21.0\n",
      "episode 4950  average reward of last 25 episode 22.76\n",
      "episode 4975  average reward of last 25 episode 21.72\n",
      "episode 5000  average reward of last 25 episode 21.0\n",
      "Saved Model\n",
      "episode 5025  average reward of last 25 episode 21.6\n",
      "episode 5050  average reward of last 25 episode 20.0\n",
      "episode 5075  average reward of last 25 episode 20.84\n",
      "episode 5100  average reward of last 25 episode 21.32\n",
      "episode 5125  average reward of last 25 episode 20.6\n",
      "episode 5150  average reward of last 25 episode 21.44\n",
      "episode 5175  average reward of last 25 episode 22.36\n",
      "episode 5200  average reward of last 25 episode 21.44\n",
      "episode 5225  average reward of last 25 episode 23.04\n",
      "episode 5250  average reward of last 25 episode 20.84\n",
      "episode 5275  average reward of last 25 episode 22.36\n",
      "episode 5300  average reward of last 25 episode 22.76\n",
      "episode 5325  average reward of last 25 episode 21.28\n",
      "episode 5350  average reward of last 25 episode 21.52\n",
      "episode 5375  average reward of last 25 episode 22.0\n",
      "episode 5400  average reward of last 25 episode 22.2\n",
      "episode 5425  average reward of last 25 episode 21.68\n",
      "episode 5450  average reward of last 25 episode 21.48\n",
      "episode 5475  average reward of last 25 episode 22.44\n",
      "episode 5500  average reward of last 25 episode 21.04\n",
      "episode 5525  average reward of last 25 episode 21.8\n",
      "episode 5550  average reward of last 25 episode 20.92\n",
      "episode 5575  average reward of last 25 episode 21.76\n",
      "episode 5600  average reward of last 25 episode 21.12\n",
      "episode 5625  average reward of last 25 episode 22.0\n",
      "episode 5650  average reward of last 25 episode 23.52\n",
      "episode 5675  average reward of last 25 episode 22.52\n",
      "episode 5700  average reward of last 25 episode 21.04\n",
      "episode 5725  average reward of last 25 episode 21.08\n",
      "episode 5750  average reward of last 25 episode 20.84\n",
      "episode 5775  average reward of last 25 episode 21.6\n",
      "episode 5800  average reward of last 25 episode 20.88\n",
      "episode 5825  average reward of last 25 episode 23.48\n",
      "episode 5850  average reward of last 25 episode 22.16\n",
      "episode 5875  average reward of last 25 episode 21.68\n",
      "episode 5900  average reward of last 25 episode 22.6\n",
      "episode 5925  average reward of last 25 episode 21.0\n",
      "episode 5950  average reward of last 25 episode 22.68\n",
      "episode 5975  average reward of last 25 episode 21.28\n",
      "episode 6000  average reward of last 25 episode 22.12\n",
      "Saved Model\n",
      "episode 6025  average reward of last 25 episode 22.52\n",
      "episode 6050  average reward of last 25 episode 22.08\n",
      "episode 6075  average reward of last 25 episode 20.6\n",
      "episode 6100  average reward of last 25 episode 22.48\n",
      "episode 6125  average reward of last 25 episode 21.76\n",
      "episode 6150  average reward of last 25 episode 23.16\n",
      "episode 6175  average reward of last 25 episode 22.04\n",
      "episode 6200  average reward of last 25 episode 22.64\n",
      "episode 6225  average reward of last 25 episode 22.96\n",
      "episode 6250  average reward of last 25 episode 21.88\n",
      "episode 6275  average reward of last 25 episode 22.4\n",
      "episode 6300  average reward of last 25 episode 22.36\n",
      "episode 6325  average reward of last 25 episode 22.16\n",
      "episode 6350  average reward of last 25 episode 21.92\n",
      "episode 6375  average reward of last 25 episode 22.52\n",
      "episode 6400  average reward of last 25 episode 21.84\n",
      "episode 6425  average reward of last 25 episode 23.12\n",
      "episode 6450  average reward of last 25 episode 19.92\n",
      "episode 6475  average reward of last 25 episode 21.56\n",
      "episode 6500  average reward of last 25 episode 21.36\n",
      "episode 6525  average reward of last 25 episode 22.92\n",
      "episode 6550  average reward of last 25 episode 23.48\n",
      "episode 6575  average reward of last 25 episode 22.2\n",
      "episode 6600  average reward of last 25 episode 23.0\n",
      "episode 6625  average reward of last 25 episode 21.6\n",
      "episode 6650  average reward of last 25 episode 22.0\n",
      "episode 6675  average reward of last 25 episode 21.2\n",
      "episode 6700  average reward of last 25 episode 21.64\n",
      "episode 6725  average reward of last 25 episode 22.6\n",
      "episode 6750  average reward of last 25 episode 22.68\n",
      "episode 6775  average reward of last 25 episode 21.72\n",
      "episode 6800  average reward of last 25 episode 22.36\n",
      "episode 6825  average reward of last 25 episode 21.72\n",
      "episode 6850  average reward of last 25 episode 20.24\n",
      "episode 6875  average reward of last 25 episode 22.32\n",
      "episode 6900  average reward of last 25 episode 23.4\n",
      "episode 6925  average reward of last 25 episode 22.48\n",
      "episode 6950  average reward of last 25 episode 22.16\n",
      "episode 6975  average reward of last 25 episode 23.08\n",
      "episode 7000  average reward of last 25 episode 21.76\n",
      "Saved Model\n",
      "episode 7025  average reward of last 25 episode 21.96\n",
      "episode 7050  average reward of last 25 episode 21.48\n",
      "episode 7075  average reward of last 25 episode 22.24\n",
      "episode 7100  average reward of last 25 episode 22.32\n",
      "episode 7125  average reward of last 25 episode 23.04\n",
      "episode 7150  average reward of last 25 episode 23.04\n",
      "episode 7175  average reward of last 25 episode 22.08\n",
      "episode 7200  average reward of last 25 episode 23.48\n",
      "episode 7225  average reward of last 25 episode 21.48\n",
      "episode 7250  average reward of last 25 episode 22.52\n",
      "episode 7275  average reward of last 25 episode 23.44\n",
      "episode 7300  average reward of last 25 episode 21.96\n",
      "episode 7325  average reward of last 25 episode 22.2\n",
      "episode 7350  average reward of last 25 episode 21.44\n",
      "episode 7375  average reward of last 25 episode 24.4\n",
      "episode 7400  average reward of last 25 episode 22.56\n",
      "episode 7425  average reward of last 25 episode 22.52\n",
      "episode 7450  average reward of last 25 episode 21.96\n",
      "episode 7475  average reward of last 25 episode 22.28\n",
      "episode 7500  average reward of last 25 episode 21.72\n",
      "episode 7525  average reward of last 25 episode 22.48\n",
      "episode 7550  average reward of last 25 episode 22.52\n",
      "episode 7575  average reward of last 25 episode 22.96\n",
      "episode 7600  average reward of last 25 episode 22.08\n",
      "episode 7625  average reward of last 25 episode 21.52\n",
      "episode 7650  average reward of last 25 episode 22.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 7675  average reward of last 25 episode 22.2\n",
      "episode 7700  average reward of last 25 episode 22.56\n",
      "episode 7725  average reward of last 25 episode 20.8\n",
      "episode 7750  average reward of last 25 episode 22.32\n",
      "episode 7775  average reward of last 25 episode 21.44\n",
      "episode 7800  average reward of last 25 episode 22.96\n",
      "episode 7825  average reward of last 25 episode 21.88\n",
      "episode 7850  average reward of last 25 episode 22.6\n",
      "episode 7875  average reward of last 25 episode 21.8\n",
      "episode 7900  average reward of last 25 episode 22.08\n",
      "episode 7925  average reward of last 25 episode 21.6\n",
      "episode 7950  average reward of last 25 episode 22.4\n",
      "episode 7975  average reward of last 25 episode 21.0\n",
      "episode 8000  average reward of last 25 episode 21.36\n",
      "Saved Model\n",
      "episode 8025  average reward of last 25 episode 21.56\n",
      "episode 8050  average reward of last 25 episode 23.28\n",
      "episode 8075  average reward of last 25 episode 22.32\n",
      "episode 8100  average reward of last 25 episode 22.32\n",
      "episode 8125  average reward of last 25 episode 22.28\n",
      "episode 8150  average reward of last 25 episode 22.12\n",
      "episode 8175  average reward of last 25 episode 22.44\n",
      "episode 8200  average reward of last 25 episode 23.44\n",
      "episode 8225  average reward of last 25 episode 23.32\n",
      "episode 8250  average reward of last 25 episode 22.64\n",
      "episode 8275  average reward of last 25 episode 21.84\n",
      "episode 8300  average reward of last 25 episode 23.8\n",
      "episode 8325  average reward of last 25 episode 22.16\n",
      "episode 8350  average reward of last 25 episode 22.96\n",
      "episode 8375  average reward of last 25 episode 22.56\n",
      "episode 8400  average reward of last 25 episode 21.56\n",
      "episode 8425  average reward of last 25 episode 23.12\n",
      "episode 8450  average reward of last 25 episode 22.28\n",
      "episode 8475  average reward of last 25 episode 22.28\n",
      "episode 8500  average reward of last 25 episode 23.24\n",
      "episode 8525  average reward of last 25 episode 21.76\n",
      "episode 8550  average reward of last 25 episode 23.24\n",
      "episode 8575  average reward of last 25 episode 21.44\n",
      "episode 8600  average reward of last 25 episode 23.76\n",
      "episode 8625  average reward of last 25 episode 22.56\n",
      "episode 8650  average reward of last 25 episode 22.4\n",
      "episode 8675  average reward of last 25 episode 22.08\n",
      "episode 8700  average reward of last 25 episode 22.96\n",
      "episode 8725  average reward of last 25 episode 23.0\n",
      "episode 8750  average reward of last 25 episode 23.2\n",
      "episode 8775  average reward of last 25 episode 22.2\n",
      "episode 8800  average reward of last 25 episode 21.12\n",
      "episode 8825  average reward of last 25 episode 21.8\n",
      "episode 8850  average reward of last 25 episode 22.04\n",
      "episode 8875  average reward of last 25 episode 23.56\n",
      "episode 8900  average reward of last 25 episode 20.36\n",
      "episode 8925  average reward of last 25 episode 22.8\n",
      "episode 8950  average reward of last 25 episode 22.48\n",
      "episode 8975  average reward of last 25 episode 23.16\n",
      "episode 9000  average reward of last 25 episode 23.32\n",
      "Saved Model\n",
      "episode 9025  average reward of last 25 episode 22.68\n",
      "episode 9050  average reward of last 25 episode 21.76\n",
      "episode 9075  average reward of last 25 episode 21.24\n",
      "episode 9100  average reward of last 25 episode 23.4\n",
      "episode 9125  average reward of last 25 episode 23.2\n",
      "episode 9150  average reward of last 25 episode 21.4\n",
      "episode 9175  average reward of last 25 episode 21.8\n",
      "episode 9200  average reward of last 25 episode 23.36\n",
      "episode 9225  average reward of last 25 episode 23.24\n",
      "episode 9250  average reward of last 25 episode 23.04\n",
      "episode 9275  average reward of last 25 episode 21.84\n",
      "episode 9300  average reward of last 25 episode 21.52\n",
      "episode 9325  average reward of last 25 episode 21.84\n",
      "episode 9350  average reward of last 25 episode 23.64\n",
      "episode 9375  average reward of last 25 episode 23.56\n",
      "episode 9400  average reward of last 25 episode 23.08\n",
      "episode 9425  average reward of last 25 episode 22.72\n",
      "episode 9450  average reward of last 25 episode 22.44\n",
      "episode 9475  average reward of last 25 episode 22.44\n",
      "episode 9500  average reward of last 25 episode 22.56\n",
      "episode 9525  average reward of last 25 episode 23.16\n",
      "episode 9550  average reward of last 25 episode 22.68\n",
      "episode 9575  average reward of last 25 episode 24.04\n",
      "episode 9600  average reward of last 25 episode 23.16\n",
      "episode 9625  average reward of last 25 episode 22.72\n",
      "episode 9650  average reward of last 25 episode 21.52\n",
      "episode 9675  average reward of last 25 episode 22.08\n",
      "episode 9700  average reward of last 25 episode 22.08\n",
      "episode 9725  average reward of last 25 episode 23.08\n",
      "episode 9750  average reward of last 25 episode 20.84\n",
      "episode 9775  average reward of last 25 episode 21.6\n",
      "episode 9800  average reward of last 25 episode 21.72\n",
      "episode 9825  average reward of last 25 episode 21.2\n",
      "episode 9850  average reward of last 25 episode 23.12\n",
      "episode 9875  average reward of last 25 episode 23.0\n",
      "episode 9900  average reward of last 25 episode 22.4\n",
      "episode 9925  average reward of last 25 episode 23.48\n",
      "episode 9950  average reward of last 25 episode 22.2\n",
      "episode 9975  average reward of last 25 episode 23.68\n",
      "episode 10000  average reward of last 25 episode 22.56\n",
      "Saved Model\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 #How many experiences to use for each training step.\n",
    "update_freq = 4 #How often to perform a training step.\n",
    "y = .99 #Discount factor on the target Q-values\n",
    "startE = 1 #Starting chance of random action\n",
    "endE = 0.1 #Final chance of random action\n",
    "anneling_steps = 10000. #How many steps of training to reduce startE to endE.\n",
    "num_episodes = 10000#How many episodes of game environment to train network with.\n",
    "pre_train_steps = 10000 #How many steps of random actions before training begins.\n",
    "max_epLength = 50 #The max allowed length of our episode.\n",
    "load_model = False #Whether to load a saved model.\n",
    "path = \"./dqn\" #The path to save our model to.\n",
    "h_size = 512 #The size of the final convolutional layer before splitting it into Advantage and Value streams.\n",
    "tau = 0.001 #Rate to update target network toward primary network\n",
    "\n",
    "tf.reset_default_graph()\n",
    "mainQN = Qnetwork(h_size)\n",
    "targetQN = Qnetwork(h_size)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "targetOps = updateTargetGraph(trainables,tau)\n",
    "\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "e = startE\n",
    "stepDrop = (startE - endE)/anneling_steps\n",
    "\n",
    "rList = []\n",
    "total_steps = 0\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    if load_model == True:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    \n",
    "    sess.run(init)\n",
    "    updateTarget(targetOps, sess)\n",
    "    \n",
    "    for i in range(num_episodes+1):\n",
    "        episodeBuffer = experience_buffer()\n",
    "        s = env.reset()\n",
    "        s = processState(s)\n",
    "        d = False\n",
    "        rAll = 0\n",
    "        j = 0\n",
    "        \n",
    "        while j < max_epLength:\n",
    "            j+=1\n",
    "            \n",
    "            #總步數小於pre_train_steps強制使用隨機Action\n",
    "            if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                a = np.random.randint(0,4)\n",
    "            \n",
    "            else:\n",
    "                a = sess.run(mainQN.predict, feed_dict={mainQN.scalerInput:[s]})[0]\n",
    "            \n",
    "            s1,r,d = env.step(a)\n",
    "            s1 = processState(s1)\n",
    "            total_steps += 1\n",
    "            episodeBuffer.add(np.reshape(np.array([s,a,r,s1,d]),[1,5]))\n",
    "            \n",
    "            if total_steps > pre_train_steps:\n",
    "                if e > endE:\n",
    "                    e -= stepDrop\n",
    "                    \n",
    "                if total_steps % (update_freq) == 0:\n",
    "                    trainBatch = myBuffer.sample(batch_size)\n",
    "                    A = sess.run(mainQN.predict, feed_dict={mainQN.scalerInput:np.vstack(trainBatch[:,3])})\n",
    "                    \n",
    "                    Q = sess.run(targetQN.Qout, feed_dict={targetQN.scalerInput:np.vstack(trainBatch[:,3])})\n",
    "                    \n",
    "                    doubleQ = Q[range(batch_size), A]\n",
    "                    targetQ = trainBatch[:,2] + y*doubleQ\n",
    "                    _ = sess.run(mainQN.updateModel, feed_dict={mainQN.scalerInput:np.vstack(trainBatch[:,0]),\n",
    "                                                                mainQN.targetQ:targetQ,\n",
    "                                                                mainQN.actions:trainBatch[:,1]})\n",
    "                    updateTarget(targetOps,sess)\n",
    "                    \n",
    "            rAll += r\n",
    "            s = s1\n",
    "                \n",
    "            if d == True:\n",
    "                break\n",
    "                    \n",
    "        myBuffer.add(episodeBuffer.buffer)\n",
    "        rList.append(rAll)\n",
    "                \n",
    "        if i>0 and i%25 == 0:\n",
    "            print('episode',i,' average reward of last 25 episode', np.mean(rList[-25:]))\n",
    "                    \n",
    "        if i>0 and i % 1000 == 0:\n",
    "            saver.save(sess,path+'/model-'+str(i)+'.cptk')\n",
    "            print('Saved Model')\n",
    "    saver.save(sess,path+'/model-'+str(i)+'.cptk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20bceee65c0>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20c0a71ef98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rMat = np.resize(np.array(rList),[len(rList)//100,100])\n",
    "rMean = np.average(rMat,1)\n",
    "plt.plot(rMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 2.0]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rList[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
