{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gameOb():\n",
    "    def __init__(self,coordinates,size,intensity,channel,reward,name):       \n",
    "        self.x = coordinates[0]\n",
    "        self.y = coordinates[1]\n",
    "        self.size = size\n",
    "        self.intensity = intensity\n",
    "        self.channel = channel\n",
    "        self.reward = reward\n",
    "        self.name = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gameEnv():\n",
    "    def __init__(self, size):\n",
    "        self.sizeX = size\n",
    "        self.sizeY = size\n",
    "        self.actions = 4\n",
    "        self.objects = []\n",
    "        a = self.reset()\n",
    "        plt.imshow(a, interpolation=\"nearest\")\n",
    "        \n",
    "    def reset(self):\n",
    "        self.objects = []\n",
    "        hero = gameOb(self.newPosition(),1,1,2,None,'hero')\n",
    "        self.objects.append(hero)\n",
    "        \n",
    "        goal = gameOb(self.newPosition(),1,1,1,1,'goal')\n",
    "        self.objects.append(goal)\n",
    "        \n",
    "        hole = gameOb(self.newPosition(),1,1,0,-1,'fire')\n",
    "        self.objects.append(hole)\n",
    "        \n",
    "        goal2 = gameOb(self.newPosition(),1,1,1,1,'goal')\n",
    "        self.objects.append(goal2)\n",
    "        \n",
    "        hole2 = gameOb(self.newPosition(),1,1,0,-1,'fire')\n",
    "        self.objects.append(hole2)\n",
    "        \n",
    "        goal3 = gameOb(self.newPosition(),1,1,1,1,'goal')\n",
    "        self.objects.append(goal3)\n",
    "        \n",
    "        goal4 = gameOb(self.newPosition(),1,1,1,1,'goal')\n",
    "        self.objects.append(goal4)\n",
    "        \n",
    "        state = self.renderEnv()\n",
    "        self.state = state\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def moveChar(self,direction):\n",
    "        hero = self.objects[0]\n",
    "        heroX = hero.x\n",
    "        heroY = hero.y\n",
    "        \n",
    "        if direction == 0 and hero.y >= 1:\n",
    "            hero.y -= 1\n",
    "        if direction == 1 and hero.y <= self.sizeY - 2:\n",
    "            hero.y += 1\n",
    "        if direction == 2 and hero.x >= 1:\n",
    "            hero.x -= 1\n",
    "        if direction == 3 and hero.x <= self.sizeX - 2:\n",
    "            hero.x += 1\n",
    "            \n",
    "    def newPosition(self):\n",
    "        iterables = [ range(self.sizeX), range(self.sizeY)]\n",
    "        points = []\n",
    "        \n",
    "        for t in itertools.product(*iterables):\n",
    "            points.append(t)\n",
    "        currentPositions = []\n",
    "        \n",
    "        for objectA in self.objects:\n",
    "            if(objectA.x, objectA.y) not in currentPositions:\n",
    "                currentPositions.append((objectA.x, objectA.y))\n",
    "                \n",
    "        for pos in currentPositions:\n",
    "            points.remove(pos)\n",
    "        \n",
    "        location = np.random.choice(range(len(points)), replace=False)\n",
    "        return points[location]\n",
    "    \n",
    "    def checkGoal(self):\n",
    "        others = []\n",
    "        for obj in self.objects:\n",
    "            if obj.name == 'hero':\n",
    "                hero = obj\n",
    "            else:\n",
    "                others.append(obj)\n",
    "        \n",
    "        for other in others:\n",
    "            if hero.x == other.x and hero.y == other.y:\n",
    "                self.objects.remove(other)\n",
    "                if other.reward == 1:\n",
    "                    self.objects.append(gameOb(self.newPosition(),1,1,1,1,'goal'))\n",
    "                else:\n",
    "                    self.objects.append(gameOb(self.newPosition(),1,1,0,-1,'fire'))\n",
    "                    \n",
    "                return other.reward,False\n",
    "        return 0.0,False\n",
    "    \n",
    "    def renderEnv(self):\n",
    "        a = np.ones([self.sizeY+2,self.sizeX+2,3])\n",
    "        a[1:-1,1:-1,:] = 0\n",
    "        hero = None\n",
    "        \n",
    "        for item in self.objects:\n",
    "            a[item.y+1:item.y+item.size+1, item.x+1:item.x+item.size+1, item.channel] = item.intensity\n",
    "            \n",
    "        b = scipy.misc.imresize(a[:,:,0], [84,84,1], interp='nearest')\n",
    "        c = scipy.misc.imresize(a[:,:,1], [84,84,1], interp='nearest')\n",
    "        d = scipy.misc.imresize(a[:,:,2], [84,84,1], interp='nearest')\n",
    "        a = np.stack([b,c,d],axis=2)\n",
    "        \n",
    "        return a\n",
    "    \n",
    "    def step(self,action):\n",
    "        self.moveChar(action)\n",
    "        reward,done = self.checkGoal()\n",
    "        state = self.renderEnv()\n",
    "        return state,reward,done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Philip\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:97: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "C:\\Users\\Philip\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:98: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "C:\\Users\\Philip\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:99: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18e297bdd68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gameEnv(size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self,h_size):\n",
    "        self.scalerInput = tf.placeholder(tf.float32, shape=[None,21168])\n",
    "        self.imageIn = tf.reshape(self.scalerInput, shape=[-1,84,84,3])\n",
    "        \n",
    "        self.conv1 = tf.contrib.layers.convolution2d(inputs=self.imageIn, num_outputs=32,\n",
    "                                                     kernel_size=[8,8], stride=[4,4], padding='VALID', biases_initializer=None)\n",
    "        self.conv2 = tf.contrib.layers.convolution2d(inputs=self.conv1, num_outputs=64,\n",
    "                                                     kernel_size=[4,4], stride=[2,2], padding='VALID', biases_initializer=None)\n",
    "        self.conv3 = tf.contrib.layers.convolution2d(inputs=self.conv2, num_outputs=64,\n",
    "                                                     kernel_size=[3,3], stride=[1,1], padding='VALID', biases_initializer=None)\n",
    "        self.conv4 = tf.contrib.layers.convolution2d(inputs=self.conv3, num_outputs=512,\n",
    "                                                     kernel_size=[7,7], stride=[1,1], padding='VALID', biases_initializer=None)\n",
    "        \n",
    "        #將conv4拆成DQN的Advantage Function和Value Function\n",
    "        self.streamAC, self.streamVC = tf.split(self.conv4, 2, 3) #第二個參數是要拆幾段，第三個是要拆第幾個維度\n",
    "        self.streamA = tf.contrib.layers.flatten(self.streamAC) \n",
    "        self.streamV = tf.contrib.layers.flatten(self.streamVC)\n",
    "        \n",
    "        #創建Fully connect\n",
    "        self.AW = tf.Variable(tf.random_normal([h_size//2, env.actions]))   #Action數量\n",
    "        self.VW = tf.Variable(tf.random_normal([h_size//2, 1]))\n",
    "        self.Advantage = tf.matmul(self.streamA, self.AW)\n",
    "        self.Value = tf.matmul(self.streamV, self.VW)\n",
    "        \n",
    "        #Q值\n",
    "        self.Qout = self.Value + tf.subtract(self.Advantage, tf.reduce_mean(self.Advantage, reduction_indices=1,keep_dims=True))\n",
    "        self.predict = tf.argmax(self.Qout, 1)\n",
    "        \n",
    "        self.targetQ = tf.placeholder(tf.float32,shape=[None])\n",
    "        self.actions = tf.placeholder(tf.int32,shape=[None])\n",
    "        self.action_onehot = tf.one_hot(self.actions, env.actions, dtype=tf.float32)\n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.action_onehot), reduction_indices=1)\n",
    "        \n",
    "        #loss計算targetQ跟Q的均方誤差\n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        self.loss = tf.reduce_mean(self.td_error)\n",
    "        self.trainer = tf.train.AdamOptimizer(0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "        \n",
    "    def add(self, experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer)) - self.buffer_size] = []\n",
    "            \n",
    "        self.buffer.extend(experience)\n",
    "        \n",
    "    def sample(self, size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer, size)), [size,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processState(states):\n",
    "    return np.reshape(states,[21168])\n",
    "    \n",
    "#更新Targer DQN\n",
    "def updateTargetGraph(tfVars, tau):\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "        \n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(tfVars[idx + total_vars//2].assign((var.value() * tau) + \\\n",
    "                                                                ((1 - tau) * tfVars[idx+total_vars//2].value())))\n",
    "    return op_holder\n",
    "    \n",
    "def updateTarget(op_holder, sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Philip\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:97: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "C:\\Users\\Philip\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:98: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "C:\\Users\\Philip\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:99: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 25  average reward of last 25 episode 2.08\n",
      "episode 50  average reward of last 25 episode 2.16\n",
      "episode 75  average reward of last 25 episode 1.52\n",
      "episode 100  average reward of last 25 episode 3.04\n",
      "episode 125  average reward of last 25 episode 2.8\n",
      "episode 150  average reward of last 25 episode 2.04\n",
      "episode 175  average reward of last 25 episode 2.28\n",
      "episode 200  average reward of last 25 episode 1.8\n",
      "episode 225  average reward of last 25 episode 1.08\n",
      "episode 250  average reward of last 25 episode 2.52\n",
      "episode 275  average reward of last 25 episode 1.92\n",
      "episode 300  average reward of last 25 episode 1.6\n",
      "episode 325  average reward of last 25 episode 1.24\n",
      "episode 350  average reward of last 25 episode 0.96\n",
      "episode 375  average reward of last 25 episode 0.76\n",
      "episode 400  average reward of last 25 episode 0.72\n",
      "episode 425  average reward of last 25 episode 0.08\n",
      "episode 450  average reward of last 25 episode 0.24\n",
      "episode 475  average reward of last 25 episode 0.52\n",
      "episode 500  average reward of last 25 episode 0.84\n",
      "episode 525  average reward of last 25 episode 0.36\n",
      "episode 550  average reward of last 25 episode 1.04\n",
      "episode 575  average reward of last 25 episode 0.36\n",
      "episode 600  average reward of last 25 episode 0.76\n",
      "episode 625  average reward of last 25 episode 0.16\n",
      "episode 650  average reward of last 25 episode 0.36\n",
      "episode 675  average reward of last 25 episode 0.2\n",
      "episode 700  average reward of last 25 episode 0.64\n",
      "episode 725  average reward of last 25 episode 0.4\n",
      "episode 750  average reward of last 25 episode 0.24\n",
      "episode 775  average reward of last 25 episode 0.4\n",
      "episode 800  average reward of last 25 episode 0.48\n",
      "episode 825  average reward of last 25 episode 0.2\n",
      "episode 850  average reward of last 25 episode 0.52\n",
      "episode 875  average reward of last 25 episode 0.64\n",
      "episode 900  average reward of last 25 episode 0.36\n",
      "episode 925  average reward of last 25 episode 0.72\n",
      "episode 950  average reward of last 25 episode 0.2\n",
      "episode 975  average reward of last 25 episode 0.44\n",
      "episode 1000  average reward of last 25 episode 0.36\n",
      "Saved Model\n",
      "episode 1025  average reward of last 25 episode 0.48\n",
      "episode 1050  average reward of last 25 episode 1.2\n",
      "episode 1075  average reward of last 25 episode 0.36\n",
      "episode 1100  average reward of last 25 episode -0.12\n",
      "episode 1125  average reward of last 25 episode 0.44\n",
      "episode 1150  average reward of last 25 episode 0.08\n",
      "episode 1175  average reward of last 25 episode 0.68\n",
      "episode 1200  average reward of last 25 episode 0.52\n",
      "episode 1225  average reward of last 25 episode 0.72\n",
      "episode 1250  average reward of last 25 episode 0.28\n",
      "episode 1275  average reward of last 25 episode 0.44\n",
      "episode 1300  average reward of last 25 episode 0.52\n",
      "episode 1325  average reward of last 25 episode 0.24\n",
      "episode 1350  average reward of last 25 episode 0.64\n",
      "episode 1375  average reward of last 25 episode 0.68\n",
      "episode 1400  average reward of last 25 episode 0.24\n",
      "episode 1425  average reward of last 25 episode 0.56\n",
      "episode 1450  average reward of last 25 episode 0.52\n",
      "episode 1475  average reward of last 25 episode 0.24\n",
      "episode 1500  average reward of last 25 episode 0.4\n",
      "episode 1525  average reward of last 25 episode 0.68\n",
      "episode 1550  average reward of last 25 episode 0.56\n",
      "episode 1575  average reward of last 25 episode -0.04\n",
      "episode 1600  average reward of last 25 episode 0.48\n",
      "episode 1625  average reward of last 25 episode 0.48\n",
      "episode 1650  average reward of last 25 episode 0.44\n",
      "episode 1675  average reward of last 25 episode 0.4\n",
      "episode 1700  average reward of last 25 episode 0.32\n",
      "episode 1725  average reward of last 25 episode 0.76\n",
      "episode 1750  average reward of last 25 episode 0.24\n",
      "episode 1775  average reward of last 25 episode 0.0\n",
      "episode 1800  average reward of last 25 episode 0.68\n",
      "episode 1825  average reward of last 25 episode 0.64\n",
      "episode 1850  average reward of last 25 episode 0.44\n",
      "episode 1875  average reward of last 25 episode 0.72\n",
      "episode 1900  average reward of last 25 episode 0.68\n",
      "episode 1925  average reward of last 25 episode 0.4\n",
      "episode 1950  average reward of last 25 episode 0.52\n",
      "episode 1975  average reward of last 25 episode 0.68\n",
      "episode 2000  average reward of last 25 episode 0.0\n",
      "Saved Model\n",
      "episode 2025  average reward of last 25 episode 0.0\n",
      "episode 2050  average reward of last 25 episode -0.08\n",
      "episode 2075  average reward of last 25 episode 0.16\n",
      "episode 2100  average reward of last 25 episode 0.44\n",
      "episode 2125  average reward of last 25 episode 0.72\n",
      "episode 2150  average reward of last 25 episode 0.72\n",
      "episode 2175  average reward of last 25 episode 0.8\n",
      "episode 2200  average reward of last 25 episode 0.52\n",
      "episode 2225  average reward of last 25 episode 0.6\n",
      "episode 2250  average reward of last 25 episode 1.0\n",
      "episode 2275  average reward of last 25 episode 0.28\n",
      "episode 2300  average reward of last 25 episode 0.44\n",
      "episode 2325  average reward of last 25 episode 0.12\n",
      "episode 2350  average reward of last 25 episode 0.6\n",
      "episode 2375  average reward of last 25 episode 0.52\n",
      "episode 2400  average reward of last 25 episode 0.56\n",
      "episode 2425  average reward of last 25 episode 0.64\n",
      "episode 2450  average reward of last 25 episode 0.32\n",
      "episode 2475  average reward of last 25 episode 0.32\n",
      "episode 2500  average reward of last 25 episode 0.16\n",
      "episode 2525  average reward of last 25 episode 0.44\n",
      "episode 2550  average reward of last 25 episode 0.36\n",
      "episode 2575  average reward of last 25 episode 0.6\n",
      "episode 2600  average reward of last 25 episode 0.36\n",
      "episode 2625  average reward of last 25 episode 0.36\n",
      "episode 2650  average reward of last 25 episode 0.2\n",
      "episode 2675  average reward of last 25 episode 0.16\n",
      "episode 2700  average reward of last 25 episode 0.08\n",
      "episode 2725  average reward of last 25 episode 0.56\n",
      "episode 2750  average reward of last 25 episode 0.0\n",
      "episode 2775  average reward of last 25 episode 0.08\n",
      "episode 2800  average reward of last 25 episode 0.4\n",
      "episode 2825  average reward of last 25 episode 0.6\n",
      "episode 2850  average reward of last 25 episode 0.64\n",
      "episode 2875  average reward of last 25 episode 0.48\n",
      "episode 2900  average reward of last 25 episode 0.84\n",
      "episode 2925  average reward of last 25 episode 0.28\n",
      "episode 2950  average reward of last 25 episode 0.64\n",
      "episode 2975  average reward of last 25 episode 0.76\n",
      "episode 3000  average reward of last 25 episode 0.72\n",
      "Saved Model\n",
      "episode 3025  average reward of last 25 episode 0.08\n",
      "episode 3050  average reward of last 25 episode 0.0\n",
      "episode 3075  average reward of last 25 episode 0.36\n",
      "episode 3100  average reward of last 25 episode 0.4\n",
      "episode 3125  average reward of last 25 episode 0.64\n",
      "episode 3150  average reward of last 25 episode 0.4\n",
      "episode 3175  average reward of last 25 episode 0.36\n",
      "episode 3200  average reward of last 25 episode 0.72\n",
      "episode 3225  average reward of last 25 episode 0.64\n",
      "episode 3250  average reward of last 25 episode 0.44\n",
      "episode 3275  average reward of last 25 episode 0.52\n",
      "episode 3300  average reward of last 25 episode 0.24\n",
      "episode 3325  average reward of last 25 episode 0.56\n",
      "episode 3350  average reward of last 25 episode 0.2\n",
      "episode 3375  average reward of last 25 episode 0.48\n",
      "episode 3400  average reward of last 25 episode 0.6\n",
      "episode 3425  average reward of last 25 episode 0.8\n",
      "episode 3450  average reward of last 25 episode 0.6\n",
      "episode 3475  average reward of last 25 episode 0.24\n",
      "episode 3500  average reward of last 25 episode 0.44\n",
      "episode 3525  average reward of last 25 episode 0.36\n",
      "episode 3550  average reward of last 25 episode 0.36\n",
      "episode 3575  average reward of last 25 episode 0.96\n",
      "episode 3600  average reward of last 25 episode 0.56\n",
      "episode 3625  average reward of last 25 episode 0.36\n",
      "episode 3650  average reward of last 25 episode 0.52\n",
      "episode 3675  average reward of last 25 episode 0.16\n",
      "episode 3700  average reward of last 25 episode 0.24\n",
      "episode 3725  average reward of last 25 episode 0.96\n",
      "episode 3750  average reward of last 25 episode 0.72\n",
      "episode 3775  average reward of last 25 episode 0.64\n",
      "episode 3800  average reward of last 25 episode 0.08\n",
      "episode 3825  average reward of last 25 episode 0.12\n",
      "episode 3850  average reward of last 25 episode 0.32\n",
      "episode 3875  average reward of last 25 episode 0.44\n",
      "episode 3900  average reward of last 25 episode 0.24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 3925  average reward of last 25 episode 0.76\n",
      "episode 3950  average reward of last 25 episode 0.56\n",
      "episode 3975  average reward of last 25 episode 0.92\n",
      "episode 4000  average reward of last 25 episode 0.6\n",
      "Saved Model\n",
      "episode 4025  average reward of last 25 episode 0.64\n",
      "episode 4050  average reward of last 25 episode 0.12\n",
      "episode 4075  average reward of last 25 episode 0.44\n",
      "episode 4100  average reward of last 25 episode 0.96\n",
      "episode 4125  average reward of last 25 episode 0.6\n",
      "episode 4150  average reward of last 25 episode 0.52\n",
      "episode 4175  average reward of last 25 episode 0.68\n",
      "episode 4200  average reward of last 25 episode 0.48\n",
      "episode 4225  average reward of last 25 episode 0.56\n",
      "episode 4250  average reward of last 25 episode 0.8\n",
      "episode 4275  average reward of last 25 episode 0.4\n",
      "episode 4300  average reward of last 25 episode 0.76\n",
      "episode 4325  average reward of last 25 episode 0.32\n",
      "episode 4350  average reward of last 25 episode 0.4\n",
      "episode 4375  average reward of last 25 episode 0.64\n",
      "episode 4400  average reward of last 25 episode 0.48\n",
      "episode 4425  average reward of last 25 episode 0.84\n",
      "episode 4450  average reward of last 25 episode 0.04\n",
      "episode 4475  average reward of last 25 episode 0.56\n",
      "episode 4500  average reward of last 25 episode 0.24\n",
      "episode 4525  average reward of last 25 episode 0.2\n",
      "episode 4550  average reward of last 25 episode 0.68\n",
      "episode 4575  average reward of last 25 episode 0.24\n",
      "episode 4600  average reward of last 25 episode 0.68\n",
      "episode 4625  average reward of last 25 episode 1.08\n",
      "episode 4650  average reward of last 25 episode 0.44\n",
      "episode 4675  average reward of last 25 episode 0.16\n",
      "episode 4700  average reward of last 25 episode 0.4\n",
      "episode 4725  average reward of last 25 episode 0.2\n",
      "episode 4750  average reward of last 25 episode 0.68\n",
      "episode 4775  average reward of last 25 episode 0.8\n",
      "episode 4800  average reward of last 25 episode 0.4\n",
      "episode 4825  average reward of last 25 episode 0.44\n",
      "episode 4850  average reward of last 25 episode 0.2\n",
      "episode 4875  average reward of last 25 episode 0.68\n",
      "episode 4900  average reward of last 25 episode 0.44\n",
      "episode 4925  average reward of last 25 episode 0.44\n",
      "episode 4950  average reward of last 25 episode 0.08\n",
      "episode 4975  average reward of last 25 episode 0.52\n",
      "episode 5000  average reward of last 25 episode 0.52\n",
      "Saved Model\n",
      "episode 5025  average reward of last 25 episode 0.44\n",
      "episode 5050  average reward of last 25 episode 0.16\n",
      "episode 5075  average reward of last 25 episode 0.2\n",
      "episode 5100  average reward of last 25 episode 0.28\n",
      "episode 5125  average reward of last 25 episode 0.72\n",
      "episode 5150  average reward of last 25 episode 1.16\n",
      "episode 5175  average reward of last 25 episode 0.36\n",
      "episode 5200  average reward of last 25 episode 0.96\n",
      "episode 5225  average reward of last 25 episode 0.8\n",
      "episode 5250  average reward of last 25 episode 0.68\n",
      "episode 5275  average reward of last 25 episode 0.72\n",
      "episode 5300  average reward of last 25 episode 0.2\n",
      "episode 5325  average reward of last 25 episode 0.0\n",
      "episode 5350  average reward of last 25 episode 0.52\n",
      "episode 5375  average reward of last 25 episode 0.68\n",
      "episode 5400  average reward of last 25 episode 0.4\n",
      "episode 5425  average reward of last 25 episode 0.4\n",
      "episode 5450  average reward of last 25 episode 0.84\n",
      "episode 5475  average reward of last 25 episode 0.2\n",
      "episode 5500  average reward of last 25 episode 0.16\n",
      "episode 5525  average reward of last 25 episode 0.72\n",
      "episode 5550  average reward of last 25 episode 0.28\n",
      "episode 5575  average reward of last 25 episode 0.52\n",
      "episode 5600  average reward of last 25 episode 0.04\n",
      "episode 5625  average reward of last 25 episode 0.36\n",
      "episode 5650  average reward of last 25 episode 0.4\n",
      "episode 5675  average reward of last 25 episode 0.2\n",
      "episode 5700  average reward of last 25 episode 0.76\n",
      "episode 5725  average reward of last 25 episode 0.72\n",
      "episode 5750  average reward of last 25 episode 0.96\n",
      "episode 5775  average reward of last 25 episode 0.68\n",
      "episode 5800  average reward of last 25 episode 0.24\n",
      "episode 5825  average reward of last 25 episode 0.48\n",
      "episode 5850  average reward of last 25 episode 0.32\n",
      "episode 5875  average reward of last 25 episode 0.2\n",
      "episode 5900  average reward of last 25 episode 0.6\n",
      "episode 5925  average reward of last 25 episode 0.32\n",
      "episode 5950  average reward of last 25 episode 0.52\n",
      "episode 5975  average reward of last 25 episode 0.28\n",
      "episode 6000  average reward of last 25 episode 0.56\n",
      "Saved Model\n",
      "episode 6025  average reward of last 25 episode 0.44\n",
      "episode 6050  average reward of last 25 episode 0.84\n",
      "episode 6075  average reward of last 25 episode -0.04\n",
      "episode 6100  average reward of last 25 episode 0.76\n",
      "episode 6125  average reward of last 25 episode 0.56\n",
      "episode 6150  average reward of last 25 episode 0.64\n",
      "episode 6175  average reward of last 25 episode 0.24\n",
      "episode 6200  average reward of last 25 episode 0.28\n",
      "episode 6225  average reward of last 25 episode 0.88\n",
      "episode 6250  average reward of last 25 episode -0.08\n",
      "episode 6275  average reward of last 25 episode 0.44\n",
      "episode 6300  average reward of last 25 episode 0.56\n",
      "episode 6325  average reward of last 25 episode 0.4\n",
      "episode 6350  average reward of last 25 episode 0.4\n",
      "episode 6375  average reward of last 25 episode 0.44\n",
      "episode 6400  average reward of last 25 episode 0.52\n",
      "episode 6425  average reward of last 25 episode 0.52\n",
      "episode 6450  average reward of last 25 episode 0.36\n",
      "episode 6475  average reward of last 25 episode 0.56\n",
      "episode 6500  average reward of last 25 episode 0.68\n",
      "episode 6525  average reward of last 25 episode -0.08\n",
      "episode 6550  average reward of last 25 episode 0.52\n",
      "episode 6575  average reward of last 25 episode 0.48\n",
      "episode 6600  average reward of last 25 episode 0.28\n",
      "episode 6625  average reward of last 25 episode 0.28\n",
      "episode 6650  average reward of last 25 episode 0.32\n",
      "episode 6675  average reward of last 25 episode 0.52\n",
      "episode 6700  average reward of last 25 episode 0.28\n",
      "episode 6725  average reward of last 25 episode 1.0\n",
      "episode 6750  average reward of last 25 episode 0.56\n",
      "episode 6775  average reward of last 25 episode 0.84\n",
      "episode 6800  average reward of last 25 episode 0.36\n",
      "episode 6825  average reward of last 25 episode 0.24\n",
      "episode 6850  average reward of last 25 episode 0.04\n",
      "episode 6875  average reward of last 25 episode 0.12\n",
      "episode 6900  average reward of last 25 episode 0.72\n",
      "episode 6925  average reward of last 25 episode 0.24\n",
      "episode 6950  average reward of last 25 episode 0.56\n",
      "episode 6975  average reward of last 25 episode 0.4\n",
      "episode 7000  average reward of last 25 episode 0.32\n",
      "Saved Model\n",
      "episode 7025  average reward of last 25 episode 0.36\n",
      "episode 7050  average reward of last 25 episode 0.2\n",
      "episode 7075  average reward of last 25 episode 0.92\n",
      "episode 7100  average reward of last 25 episode 0.44\n",
      "episode 7125  average reward of last 25 episode 0.28\n",
      "episode 7150  average reward of last 25 episode 0.4\n",
      "episode 7175  average reward of last 25 episode 0.24\n",
      "episode 7200  average reward of last 25 episode 0.44\n",
      "episode 7225  average reward of last 25 episode 0.8\n",
      "episode 7250  average reward of last 25 episode 0.48\n",
      "episode 7275  average reward of last 25 episode 0.56\n",
      "episode 7300  average reward of last 25 episode 0.4\n",
      "episode 7325  average reward of last 25 episode 0.24\n",
      "episode 7350  average reward of last 25 episode 0.64\n",
      "episode 7375  average reward of last 25 episode 0.68\n",
      "episode 7400  average reward of last 25 episode 0.36\n",
      "episode 7425  average reward of last 25 episode 0.44\n",
      "episode 7450  average reward of last 25 episode 0.04\n",
      "episode 7475  average reward of last 25 episode 0.72\n",
      "episode 7500  average reward of last 25 episode 0.28\n",
      "episode 7525  average reward of last 25 episode 0.6\n",
      "episode 7550  average reward of last 25 episode 0.4\n",
      "episode 7575  average reward of last 25 episode 0.6\n",
      "episode 7600  average reward of last 25 episode 0.24\n",
      "episode 7625  average reward of last 25 episode 0.36\n",
      "episode 7650  average reward of last 25 episode 0.4\n",
      "episode 7675  average reward of last 25 episode 0.16\n",
      "episode 7700  average reward of last 25 episode 0.36\n",
      "episode 7725  average reward of last 25 episode 0.64\n",
      "episode 7750  average reward of last 25 episode 0.68\n",
      "episode 7775  average reward of last 25 episode 0.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 7800  average reward of last 25 episode 0.52\n",
      "episode 7825  average reward of last 25 episode 0.4\n",
      "episode 7850  average reward of last 25 episode 0.4\n",
      "episode 7875  average reward of last 25 episode 0.44\n",
      "episode 7900  average reward of last 25 episode 0.76\n",
      "episode 7925  average reward of last 25 episode 0.64\n",
      "episode 7950  average reward of last 25 episode 0.6\n",
      "episode 7975  average reward of last 25 episode 0.52\n",
      "episode 8000  average reward of last 25 episode 0.4\n",
      "Saved Model\n",
      "episode 8025  average reward of last 25 episode 0.6\n",
      "episode 8050  average reward of last 25 episode 0.28\n",
      "episode 8075  average reward of last 25 episode 0.44\n",
      "episode 8100  average reward of last 25 episode 0.32\n",
      "episode 8125  average reward of last 25 episode 0.6\n",
      "episode 8150  average reward of last 25 episode 0.2\n",
      "episode 8175  average reward of last 25 episode 0.68\n",
      "episode 8200  average reward of last 25 episode 0.68\n",
      "episode 8225  average reward of last 25 episode 0.44\n",
      "episode 8250  average reward of last 25 episode 0.56\n",
      "episode 8275  average reward of last 25 episode -0.08\n",
      "episode 8300  average reward of last 25 episode 0.2\n",
      "episode 8325  average reward of last 25 episode 0.36\n",
      "episode 8350  average reward of last 25 episode 0.24\n",
      "episode 8375  average reward of last 25 episode 0.4\n",
      "episode 8400  average reward of last 25 episode 0.52\n",
      "episode 8425  average reward of last 25 episode 0.08\n",
      "episode 8450  average reward of last 25 episode 0.24\n",
      "episode 8475  average reward of last 25 episode 0.8\n",
      "episode 8500  average reward of last 25 episode 0.52\n",
      "episode 8525  average reward of last 25 episode 0.68\n",
      "episode 8550  average reward of last 25 episode 0.64\n",
      "episode 8575  average reward of last 25 episode 0.68\n",
      "episode 8600  average reward of last 25 episode 0.8\n",
      "episode 8625  average reward of last 25 episode 0.68\n",
      "episode 8650  average reward of last 25 episode 0.52\n",
      "episode 8675  average reward of last 25 episode 0.44\n",
      "episode 8700  average reward of last 25 episode 0.56\n",
      "episode 8725  average reward of last 25 episode 0.6\n",
      "episode 8750  average reward of last 25 episode 0.76\n",
      "episode 8775  average reward of last 25 episode 0.24\n",
      "episode 8800  average reward of last 25 episode 0.48\n",
      "episode 8825  average reward of last 25 episode 0.56\n",
      "episode 8850  average reward of last 25 episode 0.6\n",
      "episode 8875  average reward of last 25 episode 0.68\n",
      "episode 8900  average reward of last 25 episode 0.52\n",
      "episode 8925  average reward of last 25 episode 0.36\n",
      "episode 8950  average reward of last 25 episode 0.52\n",
      "episode 8975  average reward of last 25 episode 0.52\n",
      "episode 9000  average reward of last 25 episode 0.32\n",
      "Saved Model\n",
      "episode 9025  average reward of last 25 episode 0.6\n",
      "episode 9050  average reward of last 25 episode 0.36\n",
      "episode 9075  average reward of last 25 episode 0.48\n",
      "episode 9100  average reward of last 25 episode 0.04\n",
      "episode 9125  average reward of last 25 episode 0.52\n",
      "episode 9150  average reward of last 25 episode 0.28\n",
      "episode 9175  average reward of last 25 episode 0.4\n",
      "episode 9200  average reward of last 25 episode 0.12\n",
      "episode 9225  average reward of last 25 episode 1.08\n",
      "episode 9250  average reward of last 25 episode 0.56\n",
      "episode 9275  average reward of last 25 episode 1.24\n",
      "episode 9300  average reward of last 25 episode 0.28\n",
      "episode 9325  average reward of last 25 episode 0.16\n",
      "episode 9350  average reward of last 25 episode 0.56\n",
      "episode 9375  average reward of last 25 episode 0.36\n",
      "episode 9400  average reward of last 25 episode 0.44\n",
      "episode 9425  average reward of last 25 episode 0.44\n",
      "episode 9450  average reward of last 25 episode 0.56\n",
      "episode 9475  average reward of last 25 episode 0.48\n",
      "episode 9500  average reward of last 25 episode 0.72\n",
      "episode 9525  average reward of last 25 episode 0.92\n",
      "episode 9550  average reward of last 25 episode 0.76\n",
      "episode 9575  average reward of last 25 episode 0.4\n",
      "episode 9600  average reward of last 25 episode 0.04\n",
      "episode 9625  average reward of last 25 episode 0.28\n",
      "episode 9650  average reward of last 25 episode 0.2\n",
      "episode 9675  average reward of last 25 episode 0.4\n",
      "episode 9700  average reward of last 25 episode 0.48\n",
      "episode 9725  average reward of last 25 episode 0.2\n",
      "episode 9750  average reward of last 25 episode 0.16\n",
      "episode 9775  average reward of last 25 episode 0.44\n",
      "episode 9800  average reward of last 25 episode 0.68\n",
      "episode 9825  average reward of last 25 episode 0.52\n",
      "episode 9850  average reward of last 25 episode 1.16\n",
      "episode 9875  average reward of last 25 episode 0.88\n",
      "episode 9900  average reward of last 25 episode 0.64\n",
      "episode 9925  average reward of last 25 episode 0.64\n",
      "episode 9950  average reward of last 25 episode 0.28\n",
      "episode 9975  average reward of last 25 episode -0.12\n",
      "episode 10000  average reward of last 25 episode 0.36\n",
      "Saved Model\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 #How many experiences to use for each training step.\n",
    "update_freq = 4 #How often to perform a training step.\n",
    "y = .99 #Discount factor on the target Q-values\n",
    "startE = 1 #Starting chance of random action\n",
    "endE = 0.1 #Final chance of random action\n",
    "anneling_steps = 10000. #How many steps of training to reduce startE to endE.\n",
    "num_episodes = 10000#How many episodes of game environment to train network with.\n",
    "pre_train_steps = 10000 #How many steps of random actions before training begins.\n",
    "max_epLength = 50 #The max allowed length of our episode.\n",
    "load_model = False #Whether to load a saved model.\n",
    "path = \"./dqn\" #The path to save our model to.\n",
    "h_size = 512 #The size of the final convolutional layer before splitting it into Advantage and Value streams.\n",
    "tau = 0.001 #Rate to update target network toward primary network\n",
    "\n",
    "mainQN = Qnetwork(h_size)\n",
    "targetQN = Qnetwork(h_size)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "targetOps = updateTargetGraph(trainables,tau)\n",
    "\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "e = startE\n",
    "stepDrop = (startE - endE)/anneling_steps\n",
    "\n",
    "rList = []\n",
    "total_steps = 0\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    if load_model == True:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    \n",
    "    sess.run(init)\n",
    "    updateTarget(targetOps, sess)\n",
    "    \n",
    "    for i in range(num_episodes+1):\n",
    "        episodeBuffer = experience_buffer()\n",
    "        s = env.reset()\n",
    "        s = processState(s)\n",
    "        d = False\n",
    "        rAll = 0\n",
    "        j = 0\n",
    "        \n",
    "        while j < max_epLength:\n",
    "            j+=1\n",
    "            \n",
    "            #總步數小於pre_train_steps強制使用隨機Action\n",
    "            if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                a = np.random.randint(0,4)\n",
    "            \n",
    "            else:\n",
    "                a = sess.run(mainQN.predict, feed_dict={mainQN.scalerInput:[s]})[0]\n",
    "            \n",
    "            s1,r,d = env.step(a)\n",
    "            s1 = processState(s1)\n",
    "            total_steps += 1\n",
    "            episodeBuffer.add(np.reshape(np.array([s,a,r,s1,d]),[1,5]))\n",
    "            \n",
    "            if total_steps > pre_train_steps:\n",
    "                if e > endE:\n",
    "                    e -= stepDrop\n",
    "                    \n",
    "                if total_steps % (update_freq) == 0:\n",
    "                    trainBatch = myBuffer.sample(batch_size)\n",
    "                    A = sess.run(mainQN.predict, feed_dict={mainQN.scalerInput:np.vstack(trainBatch[:,3])})\n",
    "                    \n",
    "                    Q = sess.run(targetQN.Qout, feed_dict={targetQN.scalerInput:np.vstack(trainBatch[:,3])})\n",
    "                    \n",
    "                    doubleQ = Q[range(batch_size), A]\n",
    "                    targetQ = trainBatch[:,2] + y*doubleQ\n",
    "                    _ = sess.run(mainQN.updateModel, feed_dict={mainQN.scalerInput:np.vstack(trainBatch[:,0]),\n",
    "                                                                mainQN.targetQ:targetQ,\n",
    "                                                                mainQN.actions:trainBatch[:,1]})\n",
    "                    updateTarget(targetOps,sess)\n",
    "                    \n",
    "            rAll += r\n",
    "            s = s1\n",
    "                \n",
    "            if d == True:\n",
    "                break\n",
    "                    \n",
    "        myBuffer.add(episodeBuffer.buffer)\n",
    "        rList.append(rAll)\n",
    "                \n",
    "        if i>0 and i%25 == 0:\n",
    "            print('episode',i,' average reward of last 25 episode', np.mean(rList[-25:]))\n",
    "                    \n",
    "        if i>0 and i % 1000 == 0:\n",
    "            saver.save(sess,path+'/model-'+str(i)+'.cptk')\n",
    "            print('Saved Model')\n",
    "    saver.save(sess,path+'/model-'+str(i)+'.cptk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x18e1cf1dd30>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18e2982cb00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rMat = np.resize(np.array(rList),[len(rList)//100,100])\n",
    "rMean = np.average(rMat,1)\n",
    "plt.plot(rMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 2.0]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rList[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
