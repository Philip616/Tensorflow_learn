{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Philip\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gameOb():\n",
    "    def __init__(self,coordinates,size,intensity,channel,reward,name):       \n",
    "        self.x = coordinates[0]\n",
    "        self.y = coordinates[1]\n",
    "        self.size = size\n",
    "        self.intensity = intensity\n",
    "        self.channel = channel\n",
    "        self.reward = reward\n",
    "        self.name = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gameEnv():\n",
    "    def __init__(self, size):\n",
    "        self.sizeX = size\n",
    "        self.sizeY = size\n",
    "        self.actions = 4\n",
    "        self.objects = []\n",
    "        a = self.reset()\n",
    "        plt.imshow(a, interpolation=\"nearest\")\n",
    "        \n",
    "    def reset(self):\n",
    "        self.objects = []\n",
    "        hero = gameOb(self.newPosition(),1,1,2,None,'hero')\n",
    "        self.objects.append(hero)\n",
    "        \n",
    "        goal = gameOb(self.newPosition(),1,1,1,1,'goal')\n",
    "        self.objects.append(goal)\n",
    "        \n",
    "        hole = gameOb(self.newPosition(),1,1,0,-1,'fire')\n",
    "        self.objects.append(hole)\n",
    "        \n",
    "        goal2 = gameOb(self.newPosition(),1,1,1,1,'goal')\n",
    "        self.objects.append(goal2)\n",
    "        \n",
    "        hole2 = gameOb(self.newPosition(),1,1,0,-1,'fire')\n",
    "        self.objects.append(hole2)\n",
    "        \n",
    "        goal3 = gameOb(self.newPosition(),1,1,1,1,'goal')\n",
    "        self.objects.append(goal3)\n",
    "        \n",
    "        goal4 = gameOb(self.newPosition(),1,1,1,1,'goal')\n",
    "        self.objects.append(goal4)\n",
    "        \n",
    "        state = self.renderEnv()\n",
    "        self.state = state\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def moveChar(self,direction):\n",
    "        hero = self.objects[0]\n",
    "        heroX = hero.x\n",
    "        heroY = hero.y\n",
    "        \n",
    "        if direction == 0 and hero.y >= 1:\n",
    "            hero.y -= 1\n",
    "        if direction == 1 and hero.y <= self.sizeY - 2:\n",
    "            hero.y += 1\n",
    "        if direction == 2 and hero.x >= 1:\n",
    "            hero.x -= 1\n",
    "        if direction == 3 and hero.x <= self.sizeX - 2:\n",
    "            hero.x += 1\n",
    "            \n",
    "    def newPosition(self):\n",
    "        iterables = [ range(self.sizeX), range(self.sizeY)]\n",
    "        points = []\n",
    "        \n",
    "        for t in itertools.product(*iterables):\n",
    "            points.append(t)\n",
    "        currentPositions = []\n",
    "        \n",
    "        for objectA in self.objects:\n",
    "            if(objectA.x, objectA.y) not in currentPositions:\n",
    "                currentPositions.append((objectA.x, objectA.y))\n",
    "                \n",
    "        for pos in currentPositions:\n",
    "            points.remove(pos)\n",
    "        \n",
    "        location = np.random.choice(range(len(points)), replace=False)\n",
    "        return points[location]\n",
    "    \n",
    "    def checkGoal(self):\n",
    "        others = []\n",
    "        for obj in self.objects:\n",
    "            if obj.name == 'hero':\n",
    "                hero = obj\n",
    "            else:\n",
    "                others.append(obj)\n",
    "        \n",
    "        for other in others:\n",
    "            if hero.x == other.x and hero.y == other.y:\n",
    "                slef.objects.remove(other)\n",
    "                if other.reward == 1:\n",
    "                    self.objects.append(gameOb(self.newPosition(),1,1,1,1,'goal'))\n",
    "                else:\n",
    "                    self.objects.append(gameOb(self.newPosition(),1,1,0,-1,'fire'))\n",
    "                    \n",
    "                return other.reward,False\n",
    "        return 0.0,False\n",
    "    \n",
    "    def renderEnv(self):\n",
    "        a = np.ones([self.sizeY+2,self.sizeX+2,3])\n",
    "        a[1:-1,1:-1,:] = 0\n",
    "        hero = None\n",
    "        \n",
    "        for item in self.objects:\n",
    "            a[item.y+1:item.y+item.size+1, item.x+1:item.x+item.size+1, item.channel] = item.intensity\n",
    "            \n",
    "        b = scipy.misc.imresize(a[:,:,0], [84,84,1], interp='nearest')\n",
    "        c = scipy.misc.imresize(a[:,:,1], [84,84,1], interp='nearest')\n",
    "        d = scipy.misc.imresize(a[:,:,2], [84,84,1], interp='nearest')\n",
    "        a = np.stack([b,c,d],axis=2)\n",
    "        \n",
    "        return a\n",
    "    \n",
    "    def step(slef,action):\n",
    "        self.moveChar(action)\n",
    "        reward,done = self.checkGoal()\n",
    "        state = self.renderEnv()\n",
    "        return state,reward,done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Philip\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:97: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "C:\\Users\\Philip\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:98: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "C:\\Users\\Philip\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:99: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2bcb5a32438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gameEnv(size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self,h_size):\n",
    "        self.scalerInput = tf.placeholder(tf.float32, shape=[None,21168])\n",
    "        self.imageIn = tf.reshape(self.scalerInput, shape=[-1,84,84,3])\n",
    "        \n",
    "        self.conv1 = tf.contrib.layers.convolution2d(inputs=self.imageIn, num_outputs=32,\n",
    "                                                     kernel_size=[8,8], stride=[4,4], padding='VALID', biases_initializer=None)\n",
    "        self.conv2 = tf.contrib.layers.convolution2d(inputs=self.conv1, num_outputs=64,\n",
    "                                                     kernel_size=[4,4], stride=[2,2], padding='VALID', biases_initializer=None)\n",
    "        self.conv3 = tf.contrib.layers.convolution2d(inputs=self.conv2, num_outputs=64,\n",
    "                                                     kernel_size=[3,3], stride=[1,1], padding='VALID', biases_initializer=None)\n",
    "        self.conv4 = tf.contrib.layers.convolution2d(inputs=self.conv3, num_outputs=512,\n",
    "                                                     kernel_size=[7,7], stride=[1,1], padding='VALID', biases_initializer=None)\n",
    "        \n",
    "        #將conv4拆成DQN的Advantage Function和Value Function\n",
    "        self.streamAC, self.streamVC = tf.split(self.conv4, 2, 3) #第二個參數是要拆幾段，第三個是要拆第幾個維度\n",
    "        self.streamA = tf.contrib.layers.flatten(self.streamAC) \n",
    "        self.streamV = tf.contrib.layers.flatten(self.streamVC)\n",
    "        \n",
    "        #創建Fully connect\n",
    "        self.AW = tf.Variable(tf.random_normal([h_size//2, env.actions]))   #Action數量\n",
    "        self.VW = tf.Variable(tf.random_normal([h_size//2, 1]))\n",
    "        self.Advantage = tf.matmul(self.streamA, self.AW)\n",
    "        self.Value = tf.matmul(self.streamV, self.VW)\n",
    "        \n",
    "        #Q值\n",
    "        self.Qout = self.Value + tf.subtract(self.Advantage, tf.reduce_mean(self.Advantage, reduction_indices=1,keep_dims=True))\n",
    "        self.predict = tf.argmax(self.Qout, 1)\n",
    "        \n",
    "        self.targetQ = tf.placeholder(tf.float32,shape=[None])\n",
    "        self.actions = tf.placeholder(tf.int32,shape=[None])\n",
    "        self.action_onehot = tf.one_hot(self.actions, env.actions, dtype=tf.float32)\n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.action_onehot), reduction_indices=1)\n",
    "        \n",
    "        #loss計算targetQ跟Q的均方誤差\n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        self.loss = tf.reduce_mean(self.td_error)\n",
    "        self.trainer = tf.train.AdamOptimizer(0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "        \n",
    "    def add(self, experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer)) - self.buffer_size] = []\n",
    "            \n",
    "        self.buffer.extend(experience)\n",
    "        \n",
    "    def sample(self, size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer, size)), [size,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processState(states):\n",
    "    return np.reshape(states,[211628])\n",
    "    \n",
    "#更新Targer DQN\n",
    "def updateTargetGraph(tfVars, tau):\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "        \n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(tfVars[idx + total_vars//2].assign((var.value() * tau) + \\\n",
    "                                                                ((1 - tau) * tfVars[idx+total_vars//2].value())))\n",
    "    return op_holder\n",
    "    \n",
    "def updateTarget(op_holder, sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32 #How many experiences to use for each training step.\n",
    "update_freq = 4 #How often to perform a training step.\n",
    "y = .99 #Discount factor on the target Q-values\n",
    "startE = 1 #Starting chance of random action\n",
    "endE = 0.1 #Final chance of random action\n",
    "anneling_steps = 10000. #How many steps of training to reduce startE to endE.\n",
    "num_episodes = 10000#How many episodes of game environment to train network with.\n",
    "pre_train_steps = 10000 #How many steps of random actions before training begins.\n",
    "max_epLength = 50 #The max allowed length of our episode.\n",
    "load_model = False #Whether to load a saved model.\n",
    "path = \"./dqn\" #The path to save our model to.\n",
    "h_size = 512 #The size of the final convolutional layer before splitting it into Advantage and Value streams.\n",
    "tau = 0.001 #Rate to update target network toward primary network\n",
    "\n",
    "mainQN = Qnetwork(h_size)\n",
    "targetQN = Qnetwork(h_size)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "targetOps = updateTargetGraph(trainables,tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
